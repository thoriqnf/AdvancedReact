{
  "_args": [
    [
      {
        "raw": "moo@^0.4.3",
        "scope": null,
        "escapedName": "moo",
        "name": "moo",
        "rawSpec": "^0.4.3",
        "spec": ">=0.4.3 <0.5.0",
        "type": "range"
      },
      "/Users/thoriqnurfaizal/Documents/GitHub/AdvancedReact/Frontend/node_modules/nearley"
    ]
  ],
  "_from": "moo@>=0.4.3 <0.5.0",
  "_id": "moo@0.4.3",
  "_inCache": true,
  "_location": "/moo",
  "_nodeVersion": "8.1.2",
  "_npmOperationalInternal": {
    "host": "s3://npm-registry-packages",
    "tmp": "tmp/moo-0.4.3.tgz_1505383665229_0.29187062778510153"
  },
  "_npmUser": {
    "name": "tjvr",
    "email": "tim@tjvr.org"
  },
  "_npmVersion": "5.4.0",
  "_phantomChildren": {},
  "_requested": {
    "raw": "moo@^0.4.3",
    "scope": null,
    "escapedName": "moo",
    "name": "moo",
    "rawSpec": "^0.4.3",
    "spec": ">=0.4.3 <0.5.0",
    "type": "range"
  },
  "_requiredBy": [
    "/nearley"
  ],
  "_resolved": "https://registry.npmjs.org/moo/-/moo-0.4.3.tgz",
  "_shasum": "3f847a26f31cf625a956a87f2b10fbc013bfd10e",
  "_shrinkwrap": null,
  "_spec": "moo@^0.4.3",
  "_where": "/Users/thoriqnurfaizal/Documents/GitHub/AdvancedReact/Frontend/node_modules/nearley",
  "author": {
    "name": "Tim Radvan",
    "email": "tim@tjvr.org"
  },
  "bugs": {
    "url": "https://github.com/tjvr/moo/issues"
  },
  "contributors": [
    {
      "name": "Nathan"
    }
  ],
  "dependencies": {},
  "description": "Optimised tokenizer/lexer generator! 🐄 Much performance. Moo!",
  "devDependencies": {
    "benchr": "^3.2.0",
    "chevrotain": "^0.27.1",
    "eslint": "^3.17.1",
    "jest": "^19.0.2",
    "lex": "^1.7.9",
    "lexing": "^0.8.0",
    "remix": "^0.1.4",
    "tokenizer2": "^2.0.0"
  },
  "directories": {},
  "dist": {
    "integrity": "sha512-gFD2xGCl8YFgGHsqJ9NKRVdwlioeW3mI1iqfLNYQOv0+6JRwG58Zk9DIGQgyIaffSYaO1xsKnMaYzzNr1KyIAw==",
    "shasum": "3f847a26f31cf625a956a87f2b10fbc013bfd10e",
    "tarball": "https://registry.npmjs.org/moo/-/moo-0.4.3.tgz"
  },
  "files": [
    "moo.js"
  ],
  "gitHead": "854c303c1abbd780852c486346204e1526c43c70",
  "homepage": "https://github.com/tjvr/moo#readme",
  "license": "BSD-3-Clause",
  "main": "moo.js",
  "maintainers": [
    {
      "name": "nfd",
      "email": "qselsp@gmail.com"
    },
    {
      "name": "tjvr",
      "email": "tim@tjvr.org"
    }
  ],
  "name": "moo",
  "optionalDependencies": {},
  "readme": "![](cow.png)\n\nMoo!\n====\n\nMoo is a highly-optimised tokenizer/lexer generator. Use it to tokenize your strings, before parsing 'em with a parser like [nearley](https://github.com/hardmath123/nearley) or whatever else you're into.\n\n* [Fast](#is-it-fast)\n* [Convenient](#usage)\n* uses [Regular Expressions](#on-regular-expressions)\n* tracks [Line Numbers](#line-numbers)\n* handles [Keywords](#keywords)\n* supports [States](#states)\n* custom [Errors](#errors)\n* is even [Iterable](#iteration)\n* has no dependencies\n* <3KB gzipped\n* Moo!\n\nIs it fast?\n-----------\n\nYup! Flying-cows-and-singed-steak fast.\n\nMoo is the fastest JS tokenizer around. It's **~2–10x** faster than most other tokenizers; it's a **couple orders of magnitude** faster than some of the slower ones.\n\nDefine your tokens **using regular expressions**. Moo will compile 'em down to a **single RegExp for performance**. It uses the new ES6 **sticky flag** where possible to make things faster; otherwise it falls back to an almost-as-efficient workaround. (For more than you ever wanted to know about this, read [adventures in the land of substrings and RegExps](http://mrale.ph/blog/2016/11/23/making-less-dart-faster.html).)\n\nYou _might_ be able to go faster still by writing your lexer by hand rather than using RegExps, but that's icky.\n\nOh, and it [avoids parsing RegExps by itself](https://hackernoon.com/the-madness-of-parsing-real-world-javascript-regexps-d9ee336df983#.2l8qu3l76). Because that would be horrible.\n\n\nUsage\n-----\n\nFirst, you need to do the needful: `$ npm install moo`, or whatever will ship this code to your computer. Alternatively, grab the `moo.js` file by itself and slap it into your web page via a `<script>` tag; moo is completely standalone.\n\nThen you can start roasting your very own lexer/tokenizer:\n\n```js\n    const moo = require('moo')\n\n    let lexer = moo.compile({\n      WS:      /[ \\t]+/,\n      comment: /\\/\\/.*?$/,\n      number:  /0|[1-9][0-9]*/,\n      string:  /\"(?:\\\\[\"\\\\]|[^\\n\"\\\\])*\"/,\n      lparen:  '(',\n      rparen:  ')',\n      keyword: ['while', 'if', 'else', 'moo', 'cows'],\n      NL:      { match: /\\n/, lineBreaks: true },\n    })\n```\n\nAnd now throw some text at it:\n\n```js\n    lexer.reset('while (10) cows\\nmoo')\n    lexer.next() // -> { type: 'keyword', value: 'while' }\n    lexer.next() // -> { type: 'WS', value: ' ' }\n    lexer.next() // -> { type: 'lparen', value: '(' }\n    lexer.next() // -> { type: 'number', value: '10' }\n    // ...\n```\n\nWhen you reach the end of Moo's internal buffer, next() will return `undefined`. You can always `reset()` it and feed it more data when that happens.\n\n\nOn Regular Expressions\n----------------------\n\nRegExps are nifty for making tokenizers, but they can be a bit of a pain. Here are some things to be aware of:\n\n* You often want to use **non-greedy quantifiers**: e.g. `*?` instead of `*`. Otherwise your tokens will be longer than you expect:\n\n    ```js\n    let lexer = moo.compile({\n      string: /\".*\"/,   // greedy quantifier *\n      // ...\n    })\n\n    lexer.reset('\"foo\" \"bar\"')\n    lexer.next() // -> { type: 'string', value: 'foo\" \"bar' }\n    ```\n    \n    Better:\n    \n    ```js\n    let lexer = moo.compile({\n      string: /\".*?\"/,   // non-greedy quantifier *?\n      // ...\n    })\n\n    lexer.reset('\"foo\" \"bar\"')\n    lexer.next() // -> { type: 'string', value: 'foo' }\n    lexer.next() // -> { type: 'space', value: ' ' }\n    lexer.next() // -> { type: 'string', value: 'bar' }\n    ```\n\n* The **order of your rules** matters. Earlier ones will take precedence.\n\n    ```js\n    moo.compile({\n        identifier:  /[a-z0-9]+/,\n        number:  /[0-9]+/,\n    }).reset('42').next() // -> { type: 'identifier', value: '42' }\n\n    moo.compile({\n        number:  /[0-9]+/,\n        identifier:  /[a-z0-9]+/,\n    }).reset('42').next() // -> { type: 'number', value: '42' }\n    ```\n\n* Moo uses **multiline RegExps**. This has a few quirks: for example, the **dot `/./` doesn't include newlines**. Use `[^]` instead if you want to match newlines too.\n\n* Since an excluding character ranges like `/[^ ]/` (which matches anything but a space) _will_ include newlines, you have to be careful not to include them by accident! In particular, the whitespace metacharacter `\\s` includes newlines.\n\n\nLine Numbers\n------------\n\nMoo tracks detailed information about the input for you.\n\nIt will track line numbers, as long as you **apply the `lineBreaks: true` option to any rules which might contain newlines**. Moo will try to warn you if you forget to do this.\n\nNote that this is `false` by default, for performance reasons: counting the number of lines in a matched token has a small cost. For optimal performance, only match newlines inside a dedicated token:\n\n```js\n    newline: {match: '\\n', lineBreaks: true},\n```\n\n\n### Token Info ###\n\nToken objects (returned from `next()`) have the following attributes:\n\n* **`type`**: the name of the group, as passed to compile.\n* **`value`**: the match contents.\n* **`offset`**: the number of bytes from the start of the buffer where the match starts.\n* **`lineBreaks`**: the number of line breaks found in the match. (Always zero if this rule has `lineBreaks: false`.)\n* **`line`**: the line number of the beginning of the match, starting from 1.\n* **`col`**: the column where the match begins, starting from 1.\n\n\n### Reset ###\n\nCalling `reset()` on your lexer will empty its internal buffer, and set the line, column, and offset counts back to their initial value.\n\nIf you don't want this, you can `save()` the state, and later pass it as the second argument to `reset()` to explicitly control the internal state of the lexer.\n\n```js\n    lexer.reset('some line\\n')\n    let info = lexer.save() // -> { line: 10 }\n    lexer.next() // -> { line: 10 }\n    lexer.next() // -> { line: 11 }\n    // ...\n    lexer.reset('a different line\\n', info)\n    lexer.next() // -> { line: 10 }\n```\n\n\nKeywords\n--------\n\nMoo makes it convenient to define literals.\n\n```js\n    moo.compile({\n      lparen:  '(',\n      rparen:  ')',\n      keyword: ['while', 'if', 'else', 'moo', 'cows'],\n    })\n```\n\nIt'll automatically compile them into regular expressions, escaping them where necessary.\n\n**Keywords** should be written using the `keywords` attribute.\n\n```js\n    moo.compile({\n      IDEN: {match: /[a-zA-Z]+/, keywords: {\n        KW: ['while', 'if', 'else', 'moo', 'cows']),\n      }},\n      SPACE: {match: /\\s+/, lineBreaks: true},\n    })\n```\n\n\n### Why? ###\n\nYou need to do this to ensure the **longest match** principle applies, even in edge cases.\n\nImagine trying to parse the input `className` with the following rules:\n\n```js\n    keyword: ['class'],\n    identifier: /[a-zA-Z]+/,\n```\n\nYou'll get _two_ tokens — `['class', 'Name']` -- which is _not_ what you want! If you swap the order of the rules, you'll fix this example; but now you'll lex `class` wrong (as an `identifier`).\n\nThe keywords helper checks matches against the list of keywords; if any of them match, it uses the type `'keyword'` instead of `'identifier'` (for this example).\n\n\n### Keyword Types ###\n\nKeywords can also have **individual types**.\n\n```js\n    let lexer = moo.compile({\n      name: {match: /[a-zA-Z]+/, keywords: {\n        'kw-class': 'class',\n        'kw-def': 'def',\n        'kw-if': 'if',\n      }},\n      // ...\n    })\n    lexer.reset('def foo')\n    lexer.next() // -> { type: 'kw-def', value: 'def' }\n    lexer.next() // space\n    lexer.next() // -> { type: 'name', value: 'foo' }\n```\n\nYou can use [itt](https://github.com/nathan/itt)'s iterator adapters to make constructing keyword objects easier:\n\n```js\n    itt(['class', 'def', 'if'])\n    .map(k => ['kw-' + k, k])\n    .toObject()\n```\n\n\nStates\n------\n\nSometimes you want your lexer to support different states. This is useful for string interpolation, for example: to tokenize `a${{c: d}}e`, you might use:\n\n```js\n    let lexer = moo.states({\n      main: {\n        strstart: {match: '`', push: 'lit'},\n        ident:    /\\w+/,\n        lbrace:   {match: '{', push: 'main'},\n        rbrace:   {match: '}', pop: true},\n        colon:    ':',\n        space:    {match: /\\s+/, lineBreaks: true},\n      },\n      lit: {\n        interp:   {match: '${', push: 'main'},\n        escape:   /\\\\./,\n        strend:   {match: '`', pop: true},\n        const:    {match: /(?:[^$`]|\\$(?!\\{))+/, lineBreaks: true},\n      },\n    })\n    // <= `a${{c: d}}e`\n    // => strstart const interp lbrace ident colon space ident rbrace rbrace const strend\n```\n\nIt's also nice to let states inherit rules from other states and be able to count things, e.g. the interpolated expression state needs a `}` rule that can tell if it's a closing brace or the end of the interpolation, but is otherwise identical to the normal expression state.\n\nTo support this, Moo allows annotating tokens with `push`, `pop` and `next`:\n\n* **`push`** moves the lexer to a new state, and pushes the old state onto the stack.\n* **`pop`** returns to a previous state, by removing one or more states from the stack.\n* **`next`** moves to a new state, but does not affect the stack.\n\n\nErrors\n------\n\nIf no token matches, Moo will throw an Error.\n\nIf you'd rather treat errors as just another kind of token, you can ask Moo to do so.\n\n```js\n    moo.compile({\n      // ...\n      myError: moo.error,\n    })\n    \n    moo.reset('invalid')\n    moo.next() // -> { type: 'myError', value: 'invalid' }\n```\n\n\nYou can have a token type that both matches tokens _and_ contains error values.\n\n```js\n    moo.compile({\n      // ...\n      myError: {match: /[\\$?`]/, error: true},\n    })\n```\n\nIf you want to throw an error from your parser, you might find `formatError` helpful. Call it with the offending token:\n\n```js\nthrow new Error(lexer.formatError(token, \"invalid syntax\"))\n```\n\nAnd it returns a string with a pretty error message.\n\n```\nError: invalid syntax at line 2 col 15:\n\n  totally valid `syntax`\n                ^\n```\n\n\nIteration\n---------\n\nIterators: we got 'em.\n\n```js\n    for (let here of lexer) {\n      // here = { type: 'number', value: '123', ... }\n    }\n```\n\nCreate an array of tokens.\n\n```js\n    let tokens = Array.from(lexer);\n```\n\nUse [itt](https://github.com/nathan/itt)'s iteration tools with Moo.\n\n```js\n    for (let [here, next] = itt(lexer).lookahead()) { // pass a number if you need more tokens\n      // enjoy!\n    }\n```\n\n\nTransform\n---------\n\nMoo doesn't allow capturing groups, but you can supply a transform function, `value()`, which will be called on the value before storing it in the Token object.\n\n```js\n    moo.compile({\n      STRING: [\n        {match: /\"\"\"[^]*?\"\"\"/, lineBreaks: true, value: x => x.slice(3, -3)},\n        {match: /\"(?:\\\\[\"\\\\rn]|[^\"\\\\])*?\"/, lineBreaks: true, value: x => x.slice(1, -1)},\n        {match: /'(?:\\\\['\\\\rn]|[^'\\\\])*?'/, lineBreaks: true, value: x => x.slice(1, -1)},\n      ],\n      // ...\n    })\n```\n\n\nContributing\n------------\n\nDo check the [FAQ](https://github.com/tjvr/moo/issues?q=label%3Aquestion).\n\nBefore submitting an issue, [remember...](https://github.com/tjvr/moo/blob/master/.github/CONTRIBUTING.md)\n\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/tjvr/moo.git"
  },
  "scripts": {
    "benchmark": "benchr test/benchmark.js",
    "lint": "eslint moo.js",
    "moo": "echo 'Mooooo!'",
    "test": "jest ."
  },
  "version": "0.4.3"
}
